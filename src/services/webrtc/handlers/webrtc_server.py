"""
WebRTC Server - Python puro com aiortc
Ultra-baixa latÃªncia com DataChannel UDP-like
Servidor age como peer direto (nÃ£o relay)
"""

import asyncio
import json
import logging
import base64
from typing import Dict, Optional, Any
import numpy as np
from datetime import datetime

from aiohttp import web, WSMsgType
from aiohttp.web_fileresponse import FileResponse
from aiortc import RTCPeerConnection, RTCSessionDescription, RTCDataChannel, RTCConfiguration, RTCIceServer
from aiortc.contrib.signaling import object_to_string, object_from_string
import aiohttp_cors
import os
from pathlib import Path
import sys

# âœ… Phase 4a: Removed deprecated sys.path manipulation
# DEPRECATED: modules.memory removed (replaced by conversation_store service)

# Importar validador de Ã¡udio da pipeline (se disponÃ­vel)
try:
    # âœ… Phase 4a: Use proper absolute imports (no sys.path manipulation)
    from src.core.audio_pipeline_validator import AudioPipelineValidator
    audio_validator_available = True
    logger.info("âœ… Validador de Ã¡udio da pipeline disponÃ­vel")
except ImportError:
    audio_validator_available = False
    logger.warning("âš ï¸ Validador de Ã¡udio da pipeline nÃ£o disponÃ­vel")

logger = logging.getLogger(__name__)


def pcm_to_wav(pcm_data: bytes, sample_rate: int = 24000, channels: int = 1, bits_per_sample: int = 16) -> bytes:
    """
    Convert PCM audio data to WAV format with proper headers

    Args:
        pcm_data: Raw PCM audio data (float32 or int16)
        sample_rate: Sample rate in Hz (default 24000)
        channels: Number of audio channels (default 1 for mono)
        bits_per_sample: Bits per sample (default 16)

    Returns:
        WAV format audio data with headers
    """
    import struct
    import io

    # Convert float32 PCM to int16 if needed
    if len(pcm_data) % 4 == 0:  # Likely float32 data
        # Convert from float32 to int16
        audio_array = np.frombuffer(pcm_data, dtype=np.float32)
        # Clamp values to [-1, 1] and convert to int16
        audio_array = np.clip(audio_array, -1.0, 1.0)
        audio_int16 = (audio_array * 32767).astype(np.int16)
        pcm_data = audio_int16.tobytes()
        bits_per_sample = 16

    # Calculate WAV header values
    byte_rate = sample_rate * channels * bits_per_sample // 8
    block_align = channels * bits_per_sample // 8
    data_size = len(pcm_data)
    file_size = 36 + data_size

    # Create WAV header
    wav_header = struct.pack('<4sI4s4sIHHIIHH4sI',
        b'RIFF',           # ChunkID
        file_size,         # ChunkSize
        b'WAVE',           # Format
        b'fmt ',           # Subchunk1ID
        16,                # Subchunk1Size (PCM = 16)
        1,                 # AudioFormat (PCM = 1)
        channels,          # NumChannels
        sample_rate,       # SampleRate
        byte_rate,         # ByteRate
        block_align,       # BlockAlign
        bits_per_sample,   # BitsPerSample
        b'data',           # Subchunk2ID
        data_size          # Subchunk2Size
    )

    # Combine header and data
    return wav_header + pcm_data


def create_wav_header(data_size: int, sample_rate: int = 24000, channels: int = 1, bits_per_sample: int = 16) -> bytes:
    """
    Create a WAV file header for given audio parameters

    Args:
        data_size: Size of audio data in bytes
        sample_rate: Sample rate in Hz
        channels: Number of audio channels
        bits_per_sample: Bits per sample

    Returns:
        WAV header as bytes
    """
    import struct

    byte_rate = sample_rate * channels * bits_per_sample // 8
    block_align = channels * bits_per_sample // 8
    file_size = 36 + data_size

    return struct.pack('<4sI4s4sIHHIIHH4sI',
        b'RIFF',           # ChunkID
        file_size,         # ChunkSize
        b'WAVE',           # Format
        b'fmt ',           # Subchunk1ID
        16,                # Subchunk1Size (PCM = 16)
        1,                 # AudioFormat (PCM = 1)
        channels,          # NumChannels
        sample_rate,       # SampleRate
        byte_rate,         # ByteRate
        block_align,       # BlockAlign
        bits_per_sample,   # BitsPerSample
        b'data',           # Subchunk2ID
        data_size          # Subchunk2Size
    )


class WebRTCServer:
    """
    Servidor WebRTC em Python puro
    Ultra-baixa latÃªncia com DataChannel nÃ£o-ordenado
    """
    
    def __init__(self, 
                 host: str = "0.0.0.0",
                 port: int = 8088,
                 ice_servers: list = None):
        """
        Inicializar servidor WebRTC
        
        Args:
            host: Host para bind
            port: Porta WebSocket
            ice_servers: Servidores STUN/TURN
        """
        self.host = host
        self.port = port
        if ice_servers is None:
            self.ice_servers = [RTCIceServer(urls=["stun:stun.l.google.com:19302"])]
        else:
            self.ice_servers = [RTCIceServer(urls=server["urls"]) for server in ice_servers]
        
        # ConexÃµes ativas
        self.peers: Dict[str, RTCPeerConnection] = {}
        self.data_channels: Dict[str, RTCDataChannel] = {}
        self.connections: Dict[str, dict] = {}  # Armazenar dados das sessÃµes WebSocket
        
        # MÃ³dulos
        self.audio_processor = None
        self.tts_module = None
        self.dev_metrics = None

        # Validador de Ã¡udio da pipeline (se disponÃ­vel e em desenvolvimento)
        self.audio_validator = None
        if audio_validator_available and os.getenv('ENVIRONMENT') == 'development':
            try:
                self.audio_validator = AudioPipelineValidator(development_mode=True)
                logger.info("âœ… Validador de Ã¡udio da pipeline inicializado em modo desenvolvimento")
            except Exception as e:
                logger.warning(f"âš ï¸ Erro ao inicializar validador de Ã¡udio: {e}")
        
        # Sistema de memÃ³ria para manter contexto
        self.memory_store = SimpleMemoryStore(
            max_sessions=100,
            max_messages_per_session=20
        )
        
        # Caminho para arquivos estÃ¡ticos do frontend
        self.frontend_build_path = Path(__file__).parent.parent.parent / "frontend" / "build"
        
        # App aiohttp
        self.app = web.Application()
        self.setup_routes()
        self.setup_cors()
        
        # EstatÃ­sticas
        self.stats = {
            "total_connections": 0,
            "active_connections": 0,
            "total_messages": 0,
            "avg_latency": 0,
            "min_latency": 999999,
            "max_latency": 0
        }

        # ConexÃµes WebSocket ativas para envio de mÃ©tricas
        self.websocket_connections = set()
        
    def set_audio_processor(self, processor) -> Any:
        """Definir processador de Ã¡udio (Ultravox)"""
        self.audio_processor = processor
        logger.info("âœ… Processador de Ã¡udio configurado")
        
    def set_tts_module(self, tts) -> Any:
        """Definir mÃ³dulo TTS"""
        self.tts_module = tts
        logger.info("âœ… MÃ³dulo TTS configurado")

    def set_dev_metrics(self, dev_metrics) -> Any:
        """Definir mÃ³dulo de mÃ©tricas de desenvolvimento"""
        self.dev_metrics = dev_metrics
        logger.info("âœ… MÃ©tricas de desenvolvimento configuradas")

    def get_voice_info(self, voice_id) -> Any:
        """Obter informaÃ§Ãµes sobre personagem/voz"""
        # Mapeamento bÃ¡sico de vozes para personagens
        voice_map = {
            # American English voices
            'af_bella': {'name': 'Bella', 'language': 'English', 'personality': 'warm and friendly'},
            'af_alloy': {'name': 'Alloy', 'language': 'English', 'personality': 'professional and clear'},
            'af_nova': {'name': 'Nova', 'language': 'English', 'personality': 'energetic and enthusiastic'},
            'am_adam': {'name': 'Adam', 'language': 'English', 'personality': 'calm and professional'},
            'am_liam': {'name': 'Liam', 'language': 'English', 'personality': 'friendly and approachable'},

            # Portuguese voices
            'pm_alex': {'name': 'Alex', 'language': 'Portuguese', 'personality': 'profissional e claro'},

            # Spanish voices
            'ef_dora': {'name': 'Dora', 'language': 'Spanish', 'personality': 'amigable y expresiva'},
            'em_alex': {'name': 'Alex', 'language': 'Spanish', 'personality': 'profesional y claro'},

            # Italian voices
            'if_sara': {'name': 'Sara', 'language': 'Italian', 'personality': 'espressiva e appassionata'},
            'im_nicola': {'name': 'Nicola', 'language': 'Italian', 'personality': 'professionale e chiaro'},

            # Chinese voices
            'zf_xiaobei': {'name': 'Xiaobei', 'language': 'Chinese', 'personality': 'å‹å¥½äº²åˆ‡'},
            'zm_yunjian': {'name': 'Yunjian', 'language': 'Chinese', 'personality': 'ä¸“ä¸šæ¸…æ™°'},
        }

        # Retornar informaÃ§Ãµes da voz ou um padrÃ£o
        return voice_map.get(voice_id, {
            'name': 'Assistant',
            'language': 'English',
            'personality': 'helpful and professional'
        })

    async def send_to_all_clients(self, message: str) -> Any:
        """Enviar mensagem para todos os clientes conectados via WebSocket"""
        if not self.websocket_connections:
            return

        dead_connections = set()
        for ws in self.websocket_connections.copy():
            try:
                await ws.send_str(message)
            except Exception as e:
                logger.debug(f"ConexÃ£o WebSocket morta removida: {e}")
                dead_connections.add(ws)

        # Remover conexÃµes mortas
        self.websocket_connections -= dead_connections
        
    def setup_routes(self) -> Any:
        """Configurar rotas HTTP/WebSocket"""
        self.app.router.add_post("/offer", self.handle_offer)
        self.app.router.add_get("/stats", self.get_stats)
        self.app.router.add_get("/health", self.health_check)
        self.app.router.add_get("/ws", self.handle_websocket)

        # Add audio converter endpoint
        from src.services.webrtc.handlers.audio_converter_endpoint import setup_audio_converter_routes
        setup_audio_converter_routes(self.app)

        # Servir arquivos estÃ¡ticos do frontend React
        self.app.router.add_static("/static", self.frontend_build_path / "static")
        self.app.router.add_get("/", self.serve_frontend)
        self.app.router.add_get("/{path:.*}", self.serve_frontend)  # Catch-all for React Router
    
    def setup_cors(self) -> Any:
        """Configurar CORS para permitir conexÃµes cross-origin"""
        cors = aiohttp_cors.setup(self.app, defaults={
            "*": aiohttp_cors.ResourceOptions(
                allow_credentials=True,
                expose_headers="*",
                allow_headers="*",
                allow_methods="*"
            )
        })
        
        # Adicionar CORS a todas as rotas
        for route in list(self.app.router.routes()):
            if not isinstance(route.resource, web.StaticResource):
                cors.add(route)
        
    async def handle_offer(self, request) -> Any:
        """
        Lidar com oferta WebRTC do cliente
        Servidor age como peer respondente
        """
        params = await request.json()
        offer = RTCSessionDescription(sdp=params["sdp"], type=params["type"])
        
        # Gerar ID de sessÃ£o
        session_id = f"peer_{datetime.now().timestamp()}_{id(request)}"
        
        # Criar peer connection
        config = RTCConfiguration(iceServers=self.ice_servers)
        pc = RTCPeerConnection(configuration=config)
        
        self.peers[session_id] = pc
        self.stats["total_connections"] += 1
        self.stats["active_connections"] = len(self.peers)
        
        logger.info(f"ğŸ”Œ Nova conexÃ£o: {session_id}")
        
        @pc.on("datachannel")
        def on_datachannel(channel: RTCDataChannel) -> Any:
            """DataChannel criado pelo cliente"""
            logger.info(f"âœ… DataChannel aberto: {channel.label}")
            self.data_channels[session_id] = channel
            
            @channel.on("message")
            async def on_message(message) -> Any:
                """Processar mensagem do DataChannel"""
                await self.handle_data_channel_message(
                    session_id, message, channel
                )
                
        @pc.on("connectionstatechange")
        async def on_connectionstatechange() -> Any:
            """Monitorar estado da conexÃ£o"""
            logger.info(f"ğŸ“¡ Estado {session_id}: {pc.connectionState}")
            
            if pc.connectionState in ["failed", "closed"]:
                await self.cleanup_peer(session_id)
                
        # Definir descriÃ§Ã£o remota (oferta)
        await pc.setRemoteDescription(offer)
        
        # Criar resposta
        answer = await pc.createAnswer()
        await pc.setLocalDescription(answer)
        
        return web.Response(
            content_type="application/json",
            text=json.dumps({
                "sdp": pc.localDescription.sdp,
                "type": pc.localDescription.type,
                "session_id": session_id
            })
        )
        
    async def handle_data_channel_message(self,
                                         session_id: str,
                                         message: Any,
                                         channel: RTCDataChannel):
        """
        Processar mensagem recebida via DataChannel
        Ultra-baixa latÃªncia com processamento direto
        """
        start_time = asyncio.get_event_loop().time()

        try:
            # Parse mensagem
            if isinstance(message, str):
                data = json.loads(message)

                # Verificar se Ã© mensagem de configuraÃ§Ã£o
                if data.get('type') == 'config':
                    await self.handle_config_message(data, session_id, channel)
                    return
                elif data.get('type') == 'voice_change':
                    await self.handle_voice_change(data, session_id, channel)
                    return
                elif data.get('type') == 'audio_binary_header':
                    # ğŸš€ OTIMIZAÃ‡ÃƒO WebRTC: Header de Ã¡udio binÃ¡rio - esperar dados binÃ¡rios
                    logger.info(f"ğŸš€ === OTIMIZAÃ‡ÃƒO WEBRTC BINÃRIA ATIVADA ===")
                    logger.info(f"   ğŸ“¦ Header recebido: {data.get('samples')} samples, {data.get('bytes')} bytes")
                    logger.info(f"   ğŸ”Š Taxa: {data.get('sampleRate')}Hz, Formato: {data.get('format')}")
                    logger.info(f"   â° Timestamp: {data.get('timestamp')}")
                    logger.info(f"   ğŸ“± Recorder: {data.get('recorder')}")

                    # Armazenar metadados no buffer da sessÃ£o
                    if session_id not in self.connections:
                        self.connections[session_id] = {'audio_buffer': {}}

                    self.connections[session_id]['audio_buffer'] = {
                        'waiting_for_binary': True,
                        'sample_rate': data.get('sampleRate', 16000),
                        'format': data.get('format', 'pcm16'),
                        'expected_samples': data.get('samples', 0),
                        'expected_bytes': data.get('bytes', 0),
                        'timestamp': data.get('timestamp'),
                        'recorder': data.get('recorder', 'unknown')
                    }

                    logger.info(f"   âœ… Aguardando {data.get('bytes')} bytes de Ã¡udio binÃ¡rio via WebRTC...")
                    return

                # Caso contrÃ¡rio, processar como Ã¡udio JSON (backward compatibility)
                audio_data = np.array(data.get("audio", []), dtype=np.float32)
                logger.info(f"ğŸ“¦ Ãudio JSON recebido: {len(audio_data)} samples")

            elif isinstance(message, bytes):
                # Verificar se estamos aguardando dados binÃ¡rios apÃ³s header
                audio_buffer = self.connections.get(session_id, {}).get('audio_buffer', {})
                if audio_buffer.get('waiting_for_binary'):
                    # ğŸš€ OTIMIZAÃ‡ÃƒO WebRTC: Processar Ã¡udio binÃ¡rio com metadados do header
                    logger.info(f"ğŸš€ === ÃUDIO WEBRTC BINÃRIO RECEBIDO ===")
                    logger.info(f"   ğŸ“Š Dados brutos: {len(message)} bytes")
                    logger.info(f"   ğŸ“¦ Esperados: {audio_buffer.get('expected_bytes', 0)} bytes")
                    logger.info(f"   ğŸ¤ Samples esperados: {audio_buffer.get('expected_samples', 0)}")
                    logger.info(f"   ğŸ”Š Taxa: {audio_buffer.get('sample_rate', 16000)}Hz")
                    logger.info(f"   ğŸ“± Recorder: {audio_buffer.get('recorder', 'unknown')}")

                    # Validar se recebemos o tamanho correto
                    expected_bytes = audio_buffer.get('expected_bytes', 0)
                    if len(message) != expected_bytes:
                        logger.warning(f"âš ï¸ TAMANHO INCONSISTENTE: Esperado {expected_bytes}, recebido {len(message)}")

                    # Converter dados binÃ¡rios diretamente para array de Ã¡udio
                    # Os dados jÃ¡ estÃ£o em int16 como enviado pelo frontend
                    audio_int16 = np.frombuffer(message, dtype=np.int16)
                    audio_data = audio_int16.astype(np.float32) / 32768.0

                    logger.info(f"   ğŸ§ Ãudio processado: {len(audio_data)} samples")
                    logger.info(f"   â±ï¸  DuraÃ§Ã£o: {len(audio_data) / audio_buffer.get('sample_rate', 16000):.2f}s")
                    logger.info(f"   âœ… WEBRTC BINÃRIO - Ultra performance!")

                    # Limpar buffer apÃ³s processar
                    self.connections[session_id]['audio_buffer'] = {}
                else:
                    # Ãudio binÃ¡rio direto (sem header) - legacy
                    audio_data = np.frombuffer(message, dtype=np.float32)
                    logger.info(f"ğŸ“¦ Ãudio binÃ¡rio direto: {len(audio_data)} samples")
            else:
                audio_data = np.array(message, dtype=np.float32)
                logger.info(f"ğŸ“¦ Ãudio array direto: {len(audio_data)} samples")
                
            logger.info(f"ğŸ¤ Ãudio recebido: {len(audio_data)} samples")
            
            # Processar com Ultravox
            response_text = ""
            response_audio = None
            
            if self.audio_processor:
                # Obter contexto da sessÃ£o
                context_messages = await self.memory_store.get_context(session_id, max_messages=10)
                context = ""
                
                if context_messages:
                    # Formatar contexto para o Ultravox
                    for msg in context_messages[-6:]:  # Ãšltimas 6 mensagens
                        if msg['role'] == 'user':
                            context += f"User: {msg['content']}\n"
                        elif msg['role'] == 'assistant':
                            context += f"Assistant: {msg['content']}\n\n"
                    logger.info(f"ğŸ§  Usando contexto de {len(context_messages)} mensagens")
                
                # Obter voice_id da sessÃ£o
                voice_id = self.connections.get(session_id, {}).get('voice_id', 'af_bella')

                # Processar Ã¡udio com contexto e voice_id
                response_text = await self.audio_processor.process_audio(
                    audio_data,
                    session_id,
                    context=context if context else None,
                    voice_id=voice_id
                )
                
                # Salvar interaÃ§Ã£o na memÃ³ria
                await self.memory_store.add_message(
                    session_id=session_id,
                    role="user",
                    content="[Ã¡udio processado]"
                )
                await self.memory_store.add_message(
                    session_id=session_id,
                    role="assistant",
                    content=response_text
                )
                
                # Gerar Ã¡udio de resposta com TTS
                if self.tts_module and response_text:
                    response_audio = await self.tts_module.synthesize(response_text)
                    
                    # TTS retorna bytes de float32, precisamos converter para int16
                    if isinstance(response_audio, bytes):
                        # Converter bytes float32 para numpy array
                        audio_float32 = np.frombuffer(response_audio, dtype=np.float32)
                        # Converter para int16
                        response_audio = (audio_float32 * 32767).astype(np.int16).tobytes()
                        logger.info(f"ğŸµ Ãudio TTS gerado: {len(response_audio)} bytes de int16")
                    elif isinstance(response_audio, np.ndarray):
                        # Se jÃ¡ for numpy array, converter diretamente
                        response_audio = (response_audio * 32767).astype(np.int16).tobytes()
                        logger.info(f"ğŸµ Ãudio TTS gerado: {len(response_audio)} bytes")
                    
                    # Converter bytes para base64 para enviar via JSON
                    import base64
                    response_audio_b64 = base64.b64encode(response_audio).decode('utf-8') if response_audio else None
                else:
                    response_audio_b64 = None
            else:
                response_text = "Processador de Ã¡udio nÃ£o configurado"
                response_audio_b64 = None
                
            # Calcular latÃªncia
            latency = (asyncio.get_event_loop().time() - start_time) * 1000
            self.update_stats(latency)
            
            # Enviar resposta via DataChannel (ultra-rÃ¡pido!)
            response = {
                "type": "response",
                "text": response_text,
                "audio": response_audio_b64,  # Enviando como base64
                "latency": latency
            }
            
            # Enviar como JSON ou binÃ¡rio
            if channel.readyState == "open":
                channel.send(json.dumps(response))
                logger.info(f"âš¡ Resposta enviada em {latency:.1f}ms")
                
            self.stats["total_messages"] += 1
            
        except Exception as e:
            logger.error(f"âŒ Erro processando Ã¡udio: {e}")
            if channel.readyState == "open":
                channel.send(json.dumps({
                    "type": "error",
                    "error": str(e)
                }))
                
    async def cleanup_peer(self, session_id: str) -> Any:
        """Limpar peer desconectado"""
        if session_id in self.peers:
            pc = self.peers[session_id]
            await pc.close()
            del self.peers[session_id]
            
        if session_id in self.data_channels:
            del self.data_channels[session_id]
            
        self.stats["active_connections"] = len(self.peers)
        logger.info(f"ğŸ§¹ Peer removido: {session_id}")
        
    async def handle_config_message(self, data: dict, session_id: str, channel: RTCDataChannel) -> Any:
        """Processar mensagem de configuraÃ§Ã£o"""
        logger.info(f"âš™ï¸ ConfiguraÃ§Ã£o recebida para {session_id}: {data}")

        # Processar voice_id se estiver presente (aceitar tanto 'voice' quanto 'voice_id')
        voice_id = data.get('voice_id') or data.get('voice')
        if voice_id and self.tts_module:
            if hasattr(self.tts_module, 'set_voice'):
                self.tts_module.set_voice(voice_id)
                logger.info(f"ğŸ”Š Voz alterada para: {voice_id}")

        # Enviar confirmaÃ§Ã£o
        response = {
            "type": "config_ack",
            "message": "âœ… ConfiguraÃ§Ã£o aplicada",
            "voice_id": voice_id if voice_id else None
        }

        if channel.readyState == "open":
            channel.send(json.dumps(response))

    async def handle_voice_change(self, data: dict, session_id: str, channel: RTCDataChannel) -> Any:
        """Processar mudanÃ§a de voz"""
        voice_id = data.get('voice_id')
        logger.info(f"ğŸ”Š SolicitaÃ§Ã£o de mudanÃ§a de voz para {session_id}: {voice_id}")

        if voice_id and self.tts_module:
            if hasattr(self.tts_module, 'set_voice'):
                self.tts_module.set_voice(voice_id)
                logger.info(f"âœ… Voz alterada para: {voice_id}")

                # Enviar confirmaÃ§Ã£o de sucesso
                response = {
                    "type": "voice_changed",
                    "voice_id": voice_id,
                    "status": "success"
                }
            else:
                logger.warning(f"âš ï¸ TTS module nÃ£o suporta mudanÃ§a de voz dinÃ¢mica")
                response = {
                    "type": "voice_changed",
                    "voice_id": voice_id,
                    "status": "unsupported"
                }
        else:
            logger.error(f"âŒ Voz invÃ¡lida ou TTS nÃ£o configurado: {voice_id}")
            response = {
                "type": "voice_changed",
                "voice_id": voice_id,
                "status": "error"
            }

        if channel.readyState == "open":
            channel.send(json.dumps(response))

    def update_stats(self, latency: float) -> Any:
        """Atualizar estatÃ­sticas de latÃªncia"""
        n = self.stats["total_messages"]
        if n > 0:
            self.stats["avg_latency"] = (
                (self.stats["avg_latency"] * (n - 1) + latency) / n
            )
        else:
            self.stats["avg_latency"] = latency
            
        self.stats["min_latency"] = min(self.stats["min_latency"], latency)
        self.stats["max_latency"] = max(self.stats["max_latency"], latency)
        
    async def get_stats(self, request) -> Any:
        """Endpoint de estatÃ­sticas"""
        return web.json_response(self.stats)
        
    async def health_check(self, request) -> Any:
        """Health check endpoint"""
        return web.json_response({
            "status": "healthy",
            "active_connections": len(self.peers),
            "uptime": datetime.now().isoformat()
        })
        
    async def handle_websocket(self, request) -> Any:
        """
        Endpoint WebSocket para compatibilidade com UltravoxChat frontend
        Processa Ã¡udio PCM diretamente via WebSocket
        """
        ws = web.WebSocketResponse()
        await ws.prepare(request)
        
        # Usar IP/porta como identificador de sessÃ£o para manter contexto
        remote = request.remote
        session_id = f"ws_{remote}_{request.headers.get('User-Agent', 'unknown')[:20]}".replace(" ", "_")
        logger.info(f"ğŸ”Œ Nova conexÃ£o WebSocket: {session_id}")
        
        # Inicializar memory store se necessÃ¡rio
        if not self.memory_store.is_initialized:
            await self.memory_store.initialize()

        self.stats["total_connections"] += 1
        self.stats["active_connections"] += 1

        # Adicionar conexÃ£o ao set para envio de mÃ©tricas
        self.websocket_connections.add(ws)

        # Armazenar informaÃ§Ãµes da sessÃ£o (incluindo voz selecionada)
        session_data = {
            'voice_id': 'af_bella',  # Voz padrÃ£o
            'system_prompt': None,
            'audio_buffer': {},  # Buffer para chunks de Ã¡udio
        }
        self.connections[session_id] = session_data
        
        try:
            async for msg in ws:
                if msg.type == WSMsgType.BINARY:
                    # Verificar se estamos aguardando dados binÃ¡rios apÃ³s header
                    audio_buffer = self.connections.get(session_id, {}).get('audio_buffer', {})
                    if audio_buffer.get('waiting_for_binary'):
                        # ğŸš€ OTIMIZAÃ‡ÃƒO: Processar Ã¡udio binÃ¡rio com metadados do header
                        start_time = asyncio.get_event_loop().time()

                        logger.info(f"ğŸš€ === ÃUDIO BINÃRIO RECEBIDO (OTIMIZAÃ‡ÃƒO) ===")
                        logger.info(f"   ğŸ“Š Dados brutos: {len(msg.data)} bytes")
                        logger.info(f"   ğŸ“¦ Esperados: {audio_buffer.get('expected_bytes', 0)} bytes")
                        logger.info(f"   ğŸ¤ Samples esperados: {audio_buffer.get('expected_samples', 0)}")
                        logger.info(f"   ğŸ”Š Taxa: {audio_buffer.get('sample_rate', 16000)}Hz")
                        logger.info(f"   ğŸ“± Recorder: {audio_buffer.get('recorder', 'unknown')}")

                        # Validar se recebemos o tamanho correto
                        expected_bytes = audio_buffer.get('expected_bytes', 0)
                        if len(msg.data) != expected_bytes:
                            logger.warning(f"âš ï¸ TAMANHO INCONSISTENTE: Esperado {expected_bytes}, recebido {len(msg.data)}")

                        # Converter dados binÃ¡rios diretamente para array de Ã¡udio
                        # Os dados jÃ¡ estÃ£o em int16 como enviado pelo frontend
                        audio_int16 = np.frombuffer(msg.data, dtype=np.int16)
                        audio_data = audio_int16.astype(np.float32) / 32768.0

                        audio_duration_sec = len(audio_data) / audio_buffer.get('sample_rate', 16000)
                        timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]

                        logger.info(f"   ğŸ§ Ãudio processado: {len(audio_data)} samples")
                        logger.info(f"   â±ï¸  DuraÃ§Ã£o: {audio_duration_sec:.2f}s")
                        logger.info(f"   âœ… SEM CONVERSÃƒO BASE64 - Dados diretos!")

                        # Limpar buffer apÃ³s processar
                        self.connections[session_id]['audio_buffer'] = {}

                    else:
                        # Se nÃ£o estamos esperando dados binÃ¡rios, ignorar mensagem binÃ¡ria nÃ£o reconhecida
                        logger.warning(f"âš ï¸ Mensagem binÃ¡ria recebida sem header prÃ©vio - ignorando {len(msg.data)} bytes")
                        continue

                    response_text = ""
                    response_audio = None

                    if self.audio_processor:
                        logger.info(f"ğŸ¤– === PROCESSAMENTO ULTRAVOX ===")
                        logger.info(f"   ğŸ”„ Processando {len(audio_data)} samples com Ultravox v0.6 8B")

                        # Obter dados da sessÃ£o
                        voice_id = self.connections.get(session_id, {}).get('voice_id', 'af_bella')

                        # Obter contexto da sessÃ£o
                        context_messages = await self.memory_store.get_context(session_id, max_messages=10)
                        context = ""

                        if context_messages:
                            # Formatar contexto para o Ultravox
                            for msg_ctx in context_messages[-6:]:  # Ãšltimas 6 mensagens
                                if msg_ctx['role'] == 'user':
                                    context += f"User: {msg_ctx['content']}\n"
                                elif msg_ctx['role'] == 'assistant':
                                    context += f"Assistant: {msg_ctx['content']}\n\n"
                            logger.info(f"   ğŸ§  Contexto: {len(context_messages)} mensagens ({len(context)} chars)")
                        else:
                            logger.info(f"   ğŸ§  Contexto: Nenhuma mensagem anterior")

                        # Validar Ã¡udio de entrada se validador disponÃ­vel
                        if self.audio_validator:
                            try:
                                validated_audio, validation_metadata = self.audio_validator.validate_user_input(
                                    audio_data,
                                    session_id=session_id,
                                    expected_language='pt'  # Configurar idioma conforme necessÃ¡rio
                                )
                                audio_data = validated_audio  # Usar Ã¡udio validado

                                # Log da validaÃ§Ã£o com Groq se disponÃ­vel
                                if 'groq_validation' in validation_metadata:
                                    groq = validation_metadata['groq_validation']
                                    if 'transcription' in groq:
                                        logger.info(f"   ğŸ¤– [Groq] TranscriÃ§Ã£o entrada: '{groq['transcription'][:50]}...'")
                                    if not groq.get('has_voice', True):
                                        logger.warning(f"   âš ï¸ [Groq] Nenhuma voz detectada no Ã¡udio de entrada")
                            except Exception as e:
                                logger.warning(f"   âš ï¸ Erro na validaÃ§Ã£o de entrada: {e}")
                                # Continuar mesmo com erro de validaÃ§Ã£o

                        # Processar com Ultravox incluindo contexto e voice_id
                        ultravox_start = asyncio.get_event_loop().time()
                        response_text = await self.audio_processor.process_audio(
                            audio_data,
                            session_id,
                            context=context if context else None,
                            voice_id=voice_id
                        )
                        ultravox_time = (asyncio.get_event_loop().time() - ultravox_start) * 1000

                        logger.info(f"   âœ… Ultravox processado em {ultravox_time:.1f}ms")
                        logger.info(f"   ğŸ“ Resposta: '{response_text[:100]}{'...' if len(response_text) > 100 else ''}'")

                        # Salvar interaÃ§Ã£o na memÃ³ria
                        # TODO: Idealmente terÃ­amos a transcriÃ§Ã£o do Ã¡udio
                        await self.memory_store.add_message(
                            session_id=session_id,
                            role="user",
                            content="[Ã¡udio processado]"
                        )
                        await self.memory_store.add_message(
                            session_id=session_id,
                            role="assistant",
                            content=response_text
                        )


                        # Gerar Ã¡udio de resposta com TTS
                        if self.tts_module and response_text:
                            logger.info(f"ğŸ”Š === GERAÃ‡ÃƒO TTS ===")
                            logger.info(f"   ğŸ“ Texto: '{response_text[:80]}{'...' if len(response_text) > 80 else ''}'")
                            logger.info(f"   ğŸ“ Tamanho texto: {len(response_text)} caracteres")

                            tts_start = asyncio.get_event_loop().time()
                            response_audio = await self.tts_module.synthesize(response_text)
                            tts_time = (asyncio.get_event_loop().time() - tts_start) * 1000

                            # Salvar Ã¡udio original para validaÃ§Ã£o antes da conversÃ£o
                            original_audio_for_validation = response_audio

                            # TTS retorna bytes de float32, precisamos converter para int16
                            if isinstance(response_audio, bytes):
                                # Converter bytes float32 para numpy array
                                audio_float32 = np.frombuffer(response_audio, dtype=np.float32)
                                audio_duration_tts = len(audio_float32) / 37800.0  # Taxa do TTS
                                # Converter para int16
                                response_audio = (audio_float32 * 32767).astype(np.int16).tobytes()
                                logger.info(f"   âœ… TTS gerado em {tts_time:.1f}ms")
                                logger.info(f"   ğŸµ Formato: Float32 â†’ Int16")
                                logger.info(f"   ğŸ“Š Dados: {len(response_audio)} bytes ({len(audio_float32)} samples)")
                                logger.info(f"   â±ï¸  DuraÃ§Ã£o Ã¡udio: {audio_duration_tts:.2f}s")
                                logger.info(f"   ğŸ”Š Taxa: 37.8kHz (TTS)")
                            elif isinstance(response_audio, np.ndarray):
                                # Se jÃ¡ for numpy array, converter diretamente
                                audio_duration_tts = len(response_audio) / 37800.0
                                response_audio = (response_audio * 32767).astype(np.int16).tobytes()
                                logger.info(f"   âœ… TTS gerado em {tts_time:.1f}ms")
                                logger.info(f"   ğŸµ Formato: NumPy â†’ Int16")
                                logger.info(f"   ğŸ“Š Dados: {len(response_audio)} bytes")
                                logger.info(f"   â±ï¸  DuraÃ§Ã£o Ã¡udio: {audio_duration_tts:.2f}s")
                            else:
                                logger.warning(f"   âš ï¸  Formato TTS nÃ£o reconhecido: {type(response_audio)}")

                            # Validar qualidade do TTS com o validador da pipeline
                            if self.audio_validator and original_audio_for_validation and response_text:
                                try:
                                    # Validar saÃ­da do TTS
                                    validated_tts, tts_metadata = self.audio_validator.validate_tts_output(
                                        original_audio_for_validation,
                                        text_input=response_text,
                                        tts_engine="http_service",
                                        voice_id=voice_id or None,
                                        session_id=session_id
                                    )

                                    # Log da validaÃ§Ã£o com Groq se disponÃ­vel
                                    if 'groq_validation' in tts_metadata:
                                        groq = tts_metadata['groq_validation']
                                        if 'transcription' in groq:
                                            logger.info(f"   ğŸ¤– [Groq] TranscriÃ§Ã£o TTS: '{groq['transcription'][:50]}...'")
                                        if 'quality_score' in groq:
                                            logger.info(f"   ğŸ¤– [Groq] Qualidade TTS: {groq['quality_score']}/5")
                                        if groq.get('quality_score', 5) < 3:
                                            logger.warning(f"   âš ï¸ [Groq] Baixa qualidade detectada no TTS")
                                except Exception as e:
                                    logger.warning(f"   âš ï¸ Erro na validaÃ§Ã£o TTS: {e}")
                                    # Continuar mesmo com erro de validaÃ§Ã£o

                            # Validar qualidade do TTS em modo desenvolvimento (mantÃ©m compatibilidade)
                            elif self.dev_metrics and original_audio_for_validation and response_text:
                                try:
                                    # Usar o Ã¡udio original em bytes para validaÃ§Ã£o
                                    if isinstance(original_audio_for_validation, (bytes, np.ndarray)):
                                        # Assumir que sÃ£o dados float32 PCM
                                        sample_rate = 37800  # Taxa do TTS
                                        voice_id_dev = None

                                        # Executar validaÃ§Ã£o assÃ­ncrona (nÃ£o bloqueia o TTS)
                                        asyncio.create_task(
                                            self.dev_metrics.validate_tts_quality(
                                                original_audio_for_validation,
                                                response_text,
                                                sample_rate,
                                                voice_id_dev
                                            )
                                        )
                                        logger.debug(f"ğŸ” [DEV] ValidaÃ§Ã£o TTS iniciada para: '{response_text[:30]}...'")
                                except Exception as e:
                                    logger.debug(f"âš ï¸ [DEV] Erro ao iniciar validaÃ§Ã£o TTS: {e}")
                        else:
                            if not self.tts_module:
                                logger.warning("âš ï¸  === TTS NÃƒO DISPONÃVEL ===")
                                logger.warning("   âŒ MÃ³dulo TTS nÃ£o configurado")
                            elif not response_text:
                                logger.warning("âš ï¸  === TTS CANCELADO ===")
                                logger.warning("   ğŸ“ Resposta vazia do Ultravox")
                    else:
                        logger.error("âŒ Processador de Ã¡udio nÃ£o configurado!")
                        response_text = "Processador de Ã¡udio nÃ£o configurado"

                    # Calcular latÃªncia
                    latency = (asyncio.get_event_loop().time() - start_time) * 1000
                    self.update_stats(latency)

                    # Enviar resposta via WebSocket no formato esperado pelo frontend
                    response = {
                        "type": "metrics",
                        "response": response_text,
                        "latency": latency
                    }

                    # Enviar texto primeiro
                    await ws.send_str(json.dumps(response))

                    # Depois enviar Ã¡udio se houver
                    audio_sent = False
                    if response_audio:
                        await ws.send_bytes(response_audio)
                        audio_sent = True

                    # Log de resumo da resposta
                    logger.info(f"ğŸ“¤ === RESPOSTA ENVIADA ===")
                    logger.info(f"   âš¡ LatÃªncia total: {latency:.1f}ms")
                    logger.info(f"   ğŸ“ Texto: {'âœ… Enviado' if response_text else 'âŒ Vazio'}")
                    logger.info(f"   ğŸµ Ãudio: {'âœ… Enviado' if audio_sent else 'âŒ NÃ£o enviado'}")
                    logger.info(f"   ğŸ‘¤ Cliente: {session_id}")
                    logger.info(f"   ğŸ”¢ Mensagem #{self.stats['total_messages'] + 1}")

                    self.stats["total_messages"] += 1
                    
                elif msg.type == WSMsgType.TEXT:
                    # Mensagem JSON recebida
                    try:
                        data = json.loads(msg.data)
                        logger.info(f"ğŸ“ Mensagem JSON recebida: {data.get('type', 'unknown')}")

                        # Handle voice change message
                        if data.get('type') == 'voice_change':
                            voice_id = data.get('voice_id')
                            if voice_id and self.tts_module:
                                # Update TTS module voice
                                if hasattr(self.tts_module, 'set_voice'):
                                    self.tts_module.set_voice(voice_id)
                                    logger.info(f"ğŸ”Š Voz alterada para: {voice_id}")
                                    # Send confirmation to client
                                    await ws.send_str(json.dumps({
                                        'type': 'voice_changed',
                                        'voice_id': voice_id,
                                        'status': 'success'
                                    }))
                                else:
                                    logger.warning(f"âš ï¸ TTS module nÃ£o suporta mudanÃ§a de voz dinÃ¢mica")
                                    await ws.send_str(json.dumps({
                                        'type': 'voice_changed',
                                        'voice_id': voice_id,
                                        'status': 'unsupported'
                                    }))
                        elif data.get('type') == 'audio_binary_header':
                            # ğŸš€ OTIMIZAÃ‡ÃƒO: Header de Ã¡udio binÃ¡rio - esperar dados binÃ¡rios
                            logger.info(f"ğŸš€ === OTIMIZAÃ‡ÃƒO BINÃRIA WEBSOCKET ATIVADA ===")
                            logger.info(f"   ğŸ“¦ Header recebido: {data.get('samples')} samples, {data.get('bytes')} bytes")
                            logger.info(f"   ğŸ”Š Taxa: {data.get('sampleRate')}Hz, Formato: {data.get('format')}")
                            logger.info(f"   â° Timestamp: {data.get('timestamp')}")
                            logger.info(f"   ğŸ“± Recorder: {data.get('recorder')}")

                            # Armazenar metadados no buffer da sessÃ£o
                            self.connections[session_id]['audio_buffer'] = {
                                'waiting_for_binary': True,
                                'sample_rate': data.get('sampleRate', 16000),
                                'format': data.get('format', 'pcm16'),
                                'expected_samples': data.get('samples', 0),
                                'expected_bytes': data.get('bytes', 0),
                                'timestamp': data.get('timestamp'),
                                'recorder': data.get('recorder', 'unknown')
                            }

                            logger.info(f"   âœ… Aguardando {data.get('bytes')} bytes de Ã¡udio binÃ¡rio...")

                        elif data.get('type') == 'audio_chunk':
                            # Decodificar dados de Ã¡udio base64
                            audio_b64 = data.get('data', '')
                            try:
                                logger.info(f"ğŸ” [DEBUG] Dados base64 recebidos: {len(audio_b64)} caracteres")

                                # Decodificar base64
                                audio_bytes = base64.b64decode(audio_b64)
                                logger.info(f"ğŸ§ Audio chunk decodificado: {len(audio_bytes)} bytes")
                                
                                # Processar como mensagem binÃ¡ria
                                start_time = asyncio.get_event_loop().time()
                                timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
                                
                                # Converter bytes para numpy array int16 -> float32
                                audio_int16 = np.frombuffer(audio_bytes, dtype=np.int16)
                                audio_data = audio_int16.astype(np.float32) / 32768.0
                                audio_duration_sec = len(audio_data) / 16000

                                # Calcular estatÃ­sticas do Ã¡udio recebido
                                audio_rms = np.sqrt(np.mean(audio_data**2))
                                audio_peak = np.abs(audio_data).max()

                                logger.info(f"ğŸ§ === REQUISIÃ‡ÃƒO ÃUDIO RECEBIDA ===")
                                logger.info(f"   â° Timestamp: {timestamp}")
                                logger.info(f"   ğŸ“Š Dados brutos: {len(audio_bytes)} bytes")
                                logger.info(f"   ğŸ“¦ Header PCM: âŒ NÃ£o (JSON)")
                                logger.info(f"   ğŸ¤ Samples Ã¡udio: {len(audio_data)}")
                                logger.info(f"   â±ï¸  DuraÃ§Ã£o: {audio_duration_sec:.2f}s")
                                logger.info(f"   ğŸ”Š Taxa: 16kHz, Int16â†’Float32")
                                logger.info(f"   ğŸ“ˆ RMS: {audio_rms:.4f}, Peak: {audio_peak:.4f}")
                                logger.info(f"   ğŸ‘¤ SessÃ£o: {session_id}")

                                # Validar qualidade do Ã¡udio
                                if audio_duration_sec < 0.5:
                                    logger.warning(f"âš ï¸ ÃUDIO MUITO CURTO: {audio_duration_sec:.2f}s (mÃ­nimo: 0.5s)")
                                    logger.warning(f"   Ultravox pode nÃ£o conseguir processar Ã¡udio tÃ£o curto!")

                                if audio_rms < 0.001:
                                    logger.warning(f"âš ï¸ ÃUDIO MUITO BAIXO: RMS={audio_rms:.6f}")
                                    logger.warning(f"   PossÃ­vel silÃªncio ou problema de captura no frontend!")

                                if audio_peak < 0.01:
                                    logger.warning(f"âš ï¸ SINAL MUITO FRACO: Peak={audio_peak:.6f}")
                                    logger.warning(f"   Verificar configuraÃ§Ã£o do microfone no frontend!")

                                # VALIDAÃ‡ÃƒO COM GROQ (desenvolvimento)
                                groq_transcription = None
                                # Importar config aqui para verificar se estÃ¡ em desenvolvimento
                                from config import get_config
                                config = get_config()
                                if self.dev_metrics and config.IS_DEVELOPMENT:
                                    try:
                                        logger.info(f"ğŸ” === VALIDAÃ‡ÃƒO GROQ (DEBUG) ===")
                                        logger.info(f"   ğŸ“Š Validando Ã¡udio: {len(audio_data)} samples @ 16kHz")
                                        logger.info(f"   â±ï¸  DuraÃ§Ã£o: {audio_duration_sec:.2f}s")

                                        # Converter Ã¡udio para WAV para transcriÃ§Ã£o
                                        import wave
                                        import tempfile
                                        import os

                                        # Criar arquivo WAV temporÃ¡rio
                                        temp_file = tempfile.NamedTemporaryFile(suffix='.wav', delete=False)
                                        temp_filename = temp_file.name

                                        try:
                                            with wave.open(temp_file, 'wb') as wav_file:
                                                wav_file.setnchannels(1)  # Mono
                                                wav_file.setsampwidth(2)  # 16-bit
                                                wav_file.setframerate(16000)
                                                # Converter float32 para int16
                                                audio_int16_temp = (audio_data * 32767).astype(np.int16)
                                                wav_file.writeframes(audio_int16_temp.tobytes())
                                            temp_file.close()

                                            # Ler o arquivo WAV criado
                                            with open(temp_filename, 'rb') as f:
                                                wav_data = f.read()

                                            # Transcrever com Groq
                                            from src.services.stt.transcription.groq_transcription import GroqTranscription
                                            groq = GroqTranscription()
                                            groq_transcription = await groq.transcribe_audio(
                                                wav_data,
                                                sample_rate=16000,
                                                language='pt'  # Usar portuguÃªs como padrÃ£o (pode ser en, es, etc)
                                            )

                                            if groq_transcription:
                                                logger.info(f"   âœ… GROQ TranscriÃ§Ã£o: '{groq_transcription}'")
                                                logger.info(f"   ğŸ“Š Tamanho: {len(groq_transcription)} caracteres")

                                                # Enviar transcriÃ§Ã£o para frontend
                                                await ws.send_str(json.dumps({
                                                    "type": "groq_transcription",
                                                    "text": groq_transcription,
                                                    "timestamp": datetime.now().strftime("%H:%M:%S.%f")[:-3],
                                                    "audio_duration": audio_duration_sec,
                                                    "audio_samples": len(audio_data),
                                                    "audio_rms": float(audio_rms),
                                                    "audio_peak": float(audio_peak)
                                                }))
                                            else:
                                                logger.warning(f"   âš ï¸ GROQ nÃ£o conseguiu transcrever o Ã¡udio!")
                                                logger.warning(f"   âš ï¸ PossÃ­vel problema: Ã¡udio muito baixo, silÃªncio ou ruÃ­do")
                                                logger.warning(f"   âš ï¸ RMS={audio_rms:.6f}, Peak={audio_peak:.6f}")

                                                # Enviar alerta para frontend
                                                await ws.send_str(json.dumps({
                                                    "type": "groq_error",
                                                    "error": "Ãudio nÃ£o pÃ´de ser transcrito - possÃ­vel silÃªncio ou ruÃ­do",
                                                    "timestamp": datetime.now().strftime("%H:%M:%S.%f")[:-3],
                                                    "audio_rms": float(audio_rms),
                                                    "audio_peak": float(audio_peak)
                                                }))
                                        finally:
                                            # Limpar arquivo temporÃ¡rio
                                            if os.path.exists(temp_filename):
                                                os.unlink(temp_filename)

                                    except Exception as e:
                                        logger.error(f"   âŒ Erro na validaÃ§Ã£o Groq: {e}")
                                        import traceback
                                        logger.error(traceback.format_exc())
                                        # Enviar erro para frontend
                                        await ws.send_str(json.dumps({
                                            "type": "groq_error",
                                            "error": f"Erro na transcriÃ§Ã£o: {str(e)}",
                                            "timestamp": datetime.now().strftime("%H:%M:%S.%f")[:-3]
                                        }))

                                response_text = ""
                                response_audio = None
                                
                                if self.audio_processor:
                                    logger.info(f"ğŸ¤– === PROCESSAMENTO ULTRAVOX ===")
                                    logger.info(f"   ğŸ”„ Processando {len(audio_data)} samples com Ultravox v0.6 8B")

                                    # Obter dados da sessÃ£o para personalizar resposta
                                    voice_id = self.connections.get(session_id, {}).get('voice_id', 'af_bella')
                                    voice_info = self.get_voice_info(voice_id)

                                    # Obter contexto da sessÃ£o
                                    context_messages = await self.memory_store.get_context(session_id, max_messages=10)
                                    context = ""

                                    # Adicionar instruÃ§Ã£o sobre voz/personagem ao contexto
                                    if voice_info:
                                        voice_instruction = f"You are {voice_info['name']}, speaking in {voice_info['language']}. "
                                        voice_instruction += f"Your personality is {voice_info['personality']}. "
                                        context = voice_instruction + "\n\n"
                                        logger.info(f"   ğŸ­ Personagem: {voice_info['name']} ({voice_info['language']})")

                                    if context_messages:
                                        # Formatar contexto para o Ultravox
                                        for msg in context_messages[-6:]:  # Ãšltimas 6 mensagens
                                            if msg['role'] == 'user':
                                                context += f"User: {msg['content']}\n"
                                            elif msg['role'] == 'assistant':
                                                context += f"Assistant: {msg['content']}\n\n"
                                        logger.info(f"   ğŸ§  Contexto: {len(context_messages)} mensagens ({len(context)} chars)")
                                    else:
                                        logger.info(f"   ğŸ§  Contexto: Nenhuma mensagem anterior")

                                    # Importar pipeline de conversaÃ§Ã£o
                                    from pipeline.conversation import ConversationPipeline

                                    # Obter system_prompt da sessÃ£o
                                    system_prompt = self.connections.get(session_id, {}).get('system_prompt', '')

                                    # Detectar idioma baseado na voz
                                    language = ConversationPipeline.get_language_from_voice_id(voice_id)

                                    # Formatar contexto com instruÃ§Ãµes da pipeline
                                    formatted_context = ConversationPipeline.format_context_with_instructions(
                                        context=context,
                                        language=language,
                                        custom_prompt=system_prompt
                                    )

                                    # Processar com Ultravox incluindo contexto formatado e voice_id
                                    ultravox_start = asyncio.get_event_loop().time()
                                    response_text = await self.audio_processor.process_audio(
                                        audio_data,
                                        context=formatted_context,
                                        voice_id=voice_id
                                    )
                                    ultravox_time = asyncio.get_event_loop().time() - ultravox_start
                                    
                                    logger.info(f"   âœ… Resposta: {response_text[:100]}{'...' if len(response_text) > 100 else ''}")
                                    logger.info(f"   â±ï¸  Tempo Ultravox: {ultravox_time*1000:.0f}ms")

                                    # Salvar mensagem do usuÃ¡rio com transcriÃ§Ã£o se disponÃ­vel
                                    user_message = groq_transcription if groq_transcription else "[Mensagem de Ã¡udio]"

                                    # Validar coerÃªncia da resposta do Ultravox com Groq LLM
                                    if self.audio_validator and user_message and response_text:
                                        try:
                                            llm_validation = self.audio_validator.validate_ultravox_response(
                                                user_question=user_message,
                                                ultravox_response=response_text,
                                                session_id=session_id
                                            )

                                            if 'llm_validation' in llm_validation:
                                                llm_data = llm_validation['llm_validation']
                                                if 'coherence_score' in llm_data:
                                                    logger.info(f"   ğŸ¤– === VALIDAÃ‡ÃƒO LLM DA RESPOSTA ===")
                                                    logger.info(f"      CoerÃªncia: {llm_data['coherence_score']}/10")
                                                    logger.info(f"      Completude: {llm_data['completeness_score']}/10")
                                                    logger.info(f"      Qualidade: {llm_data['quality_score']}/10")
                                                    logger.info(f"      VÃ¡lida: {'âœ…' if llm_data.get('is_valid', False) else 'âŒ'}")

                                                    if llm_data.get('issues'):
                                                        logger.warning(f"      âš ï¸ Problemas: {', '.join(llm_data['issues'])}")

                                                    # Score geral
                                                    overall_score = llm_validation.get('overall_score', 0)
                                                    if overall_score < 5:
                                                        logger.warning(f"      âš ï¸ ATENÃ‡ÃƒO: Score baixo ({overall_score:.1f}/10)")
                                                        logger.warning(f"      ğŸ“ AnÃ¡lise: {llm_data.get('analysis', '')[:150]}")
                                                    else:
                                                        logger.info(f"      âœ… Score geral: {overall_score:.1f}/10")
                                        except Exception as e:
                                            logger.debug(f"âš ï¸ Erro ao validar resposta com LLM: {e}")

                                    await self.memory_store.add_message(session_id, "user", user_message)
                                    # Salvar resposta do assistente
                                    await self.memory_store.add_message(session_id, "assistant", response_text)
                                
                                # Gerar TTS se hÃ¡ resposta
                                if response_text and response_text.strip() and self.tts_module:
                                    logger.info(f"ğŸ”Š === GERAÃ‡ÃƒO TTS ===")
                                    logger.info(f"   ğŸ“ Texto: {response_text[:100]}{'...' if len(response_text) > 100 else ''}")
                                    
                                    tts_start = asyncio.get_event_loop().time()
                                    response_audio = await self.tts_module.synthesize(response_text)
                                    tts_time = asyncio.get_event_loop().time() - tts_start
                                    
                                    if response_audio is not None:
                                        logger.info(f"   âœ… TTS gerado: {len(response_audio)} bytes")
                                        logger.info(f"   â±ï¸  Tempo TTS: {tts_time*1000:.0f}ms")
                                    else:
                                        logger.warning(f"   âŒ Falha na geraÃ§Ã£o TTS")
                                
                                # Enviar resposta via WebSocket
                                # Comprimir Ã¡udio para MP3 para reduzir tamanho
                                compressed_audio = None
                                if response_audio:
                                    try:
                                        # Verificar se Ã© bytes ou numpy array
                                        if isinstance(response_audio, bytes):
                                            # JÃ¡ estÃ¡ em bytes, apenas codificar
                                            compressed_audio = base64.b64encode(response_audio).decode('utf-8')
                                        else:
                                            # Ã‰ numpy array, converter
                                            if response_audio.dtype == np.float32:
                                                audio_int16 = (response_audio * 32767).astype(np.int16)
                                            else:
                                                audio_int16 = response_audio
                                            compressed_audio = base64.b64encode(audio_int16.tobytes()).decode('utf-8')
                                        
                                        # Se ainda for muito grande, dividir em chunks
                                        max_size = 500000  # 500KB limite seguro
                                        if len(compressed_audio) > max_size:
                                            logger.warning(f"âš ï¸ Ãudio muito grande ({len(compressed_audio)} bytes), enviando apenas primeiros {max_size} bytes")
                                            # Enviar apenas parte do Ã¡udio
                                            compressed_audio = compressed_audio[:max_size]
                                    except Exception as e:
                                        logger.error(f"âŒ Erro ao comprimir Ã¡udio: {e}")
                                        compressed_audio = None
                                
                                response = {
                                    'type': 'response',
                                    'text': response_text,
                                    'audio': compressed_audio,
                                    'timestamp': timestamp
                                }
                                
                                response_json = json.dumps(response)
                                logger.info(f"ğŸ“¤ Enviando resposta: {len(response_json)} bytes")
                                
                                if len(response_json) > 1000000:  # 1MB
                                    logger.warning(f"âš ï¸ Resposta muito grande: {len(response_json)} bytes")
                                    # Enviar sem Ã¡udio se for muito grande
                                    response = {
                                        'type': 'response',
                                        'text': response_text,
                                        'audio': None,
                                        'error': 'Audio too large',
                                        'timestamp': timestamp
                                    }
                                    await ws.send_str(json.dumps(response))
                                else:
                                    await ws.send_str(response_json)
                                
                                end_time = asyncio.get_event_loop().time()
                                total_time = end_time - start_time
                                
                                logger.info(f"ğŸ“Š === RESPOSTA ENVIADA ===")
                                logger.info(f"   â±ï¸  Tempo total: {total_time*1000:.0f}ms")
                                logger.info(f"   ğŸ“ Texto: {'âœ… Enviado' if response_text else 'âŒ NÃ£o enviado'}")
                                logger.info(f"   ğŸµ Ãudio: {'âœ… Enviado' if response_audio else 'âŒ NÃ£o enviado'}")
                                logger.info(f"   ğŸ‘¤ Cliente: {session_id}")
                                logger.info(f"   ğŸ”¢ Mensagem #{self.stats['total_messages'] + 1}")
                                
                                self.stats["total_messages"] += 1
                                
                            except Exception as e:
                                logger.error(f"âŒ Erro ao processar audio_chunk: {e}")
                        elif data.get('type') == 'config':
                            # ConfiguraÃ§Ã£o inicial do cliente
                            logger.info(f"âš™ï¸ ConfiguraÃ§Ã£o inicial do cliente recebida")

                            # Processar voice_id se estiver presente (aceitar tanto 'voice' quanto 'voice_id')
                            voice_id = data.get('voice_id') or data.get('voice')
                            system_prompt = data.get('system_prompt')

                            # Atualizar dados da sessÃ£o
                            if session_id in self.connections:
                                if voice_id:
                                    self.connections[session_id]['voice_id'] = voice_id
                                    logger.info(f"ğŸ¯ Voz da sessÃ£o atualizada para: {voice_id}")
                                if system_prompt:
                                    self.connections[session_id]['system_prompt'] = system_prompt
                                    logger.info(f"ğŸ“ System prompt atualizado para sessÃ£o")

                            # Atualizar TTS se disponÃ­vel
                            if voice_id and self.tts_module:
                                if hasattr(self.tts_module, 'set_voice'):
                                    self.tts_module.set_voice(voice_id)
                                    logger.info(f"ğŸ”Š Voz do TTS alterada para: {voice_id} via WebSocket config")

                            await ws.send_str(json.dumps({
                                'type': 'config_ack',
                                'message': 'âœ… Conectado ao servidor Ultravox+TTS',
                                'tts_enabled': True,
                                'speech_enabled': True,
                                'server_info': 'ultravox_server v1.0',
                                'voice_id': voice_id if voice_id else None
                            }))

                        elif data.get('type') == 'text_message':
                            # Mensagem de texto (como cliques de opÃ§Ã£o)
                            text = data.get('text', '')
                            timestamp = data.get('timestamp', 0)

                            logger.info(f"ğŸ’¬ === MENSAGEM DE TEXTO RECEBIDA ===")
                            logger.info(f"   ğŸ“ Texto: '{text}'")
                            logger.info(f"   â° Timestamp: {timestamp}")
                            logger.info(f"   ğŸ‘¤ SessÃ£o: {session_id}")

                            if text.strip():
                                try:
                                    # Enviar status de processamento
                                    await ws.send_str(json.dumps({
                                        'type': 'processing',
                                        'message': 'ğŸ¤– Processando sua mensagem...'
                                    }))

                                    # Obter dados da sessÃ£o
                                    voice_id = self.connections.get(session_id, {}).get('voice_id', 'af_bella')
                                    system_prompt = self.connections.get(session_id, {}).get('system_prompt', '')

                                    # Processar mensagem de texto com IA (se disponÃ­vel)
                                    response_text = ""
                                    if self.audio_processor:
                                        # Obter contexto da conversa
                                        context_messages = await self.memory_store.get_context(session_id, max_messages=10)
                                        context = ""

                                        if context_messages:
                                            for msg in context_messages[-6:]:
                                                if msg['role'] == 'user':
                                                    context += f"User: {msg['content']}\n"
                                                elif msg['role'] == 'assistant':
                                                    context += f"Assistant: {msg['content']}\n\n"

                                        # Usar pipeline de conversaÃ§Ã£o se disponÃ­vel
                                        try:
                                            from pipeline.conversation import ConversationPipeline

                                            # Detectar idioma baseado na voz
                                            language = ConversationPipeline.get_language_from_voice_id(voice_id)

                                            # Formatar contexto com instruÃ§Ãµes da pipeline
                                            formatted_context = ConversationPipeline.format_context_with_instructions(
                                                context=context,
                                                language=language,
                                                custom_prompt=system_prompt
                                            )

                                            # Processar texto diretamente com LLM
                                            response_text = await ConversationPipeline.process_text_message(
                                                text_input=text,
                                                context=formatted_context,
                                                language=language,
                                                session_id=session_id
                                            )

                                            logger.info(f"   âœ… Resposta LLM: '{response_text[:100]}{'...' if len(response_text) > 100 else ''}'")

                                        except ImportError:
                                            # Fallback simples se pipeline nÃ£o disponÃ­vel
                                            response_text = f"Entendi sua mensagem: '{text}'. Como posso ajudar mais?"
                                            logger.warning("   âš ï¸ Pipeline nÃ£o disponÃ­vel, usando resposta fallback")
                                    else:
                                        # Fallback sem processador de Ã¡udio
                                        response_text = f"Recebi sua mensagem: '{text}'. Obrigado!"

                                    # Salvar interaÃ§Ã£o na memÃ³ria
                                    await self.memory_store.add_message(session_id, "user", text)
                                    await self.memory_store.add_message(session_id, "assistant", response_text)

                                    # Gerar Ã¡udio TTS se mÃ³dulo disponÃ­vel
                                    response_audio = None
                                    if response_text and self.tts_module:
                                        logger.info(f"   ğŸ”Š Gerando TTS para resposta...")

                                        # Set voice before synthesis
                                        if hasattr(self.tts_module, 'set_voice'):
                                            self.tts_module.set_voice(voice_id)

                                        tts_start = asyncio.get_event_loop().time()
                                        response_audio = await self.tts_module.synthesize(response_text)
                                        tts_time = (asyncio.get_event_loop().time() - tts_start) * 1000

                                        if response_audio:
                                            logger.info(f"   âœ… TTS gerado: {len(response_audio)} bytes em {tts_time:.0f}ms")

                                    # Preparar resposta
                                    response_data = {
                                        'type': 'response',
                                        'text': response_text,
                                        'timestamp': datetime.now().isoformat(),
                                        'processing_info': {
                                            'input_type': 'text_message',
                                            'voice_id': voice_id,
                                            'original_text': text
                                        }
                                    }

                                    # Incluir Ã¡udio se gerado
                                    if response_audio:
                                        # Converter PCM para WAV com headers apropriados
                                        wav_audio = pcm_to_wav(response_audio, sample_rate=24000, channels=1, bits_per_sample=16)

                                        # Converter para base64
                                        audio_b64 = base64.b64encode(wav_audio).decode('utf-8')
                                        response_data['audio'] = audio_b64
                                        response_data['audio_format'] = 'wav'
                                        response_data['sample_rate'] = 24000

                                    # Enviar resposta
                                    await ws.send_str(json.dumps(response_data))

                                    logger.info(f"   âœ… Resposta enviada para clique de opÃ§Ã£o")
                                    logger.info(f"   ğŸ“ Texto: {'âœ…' if response_text else 'âŒ'}")
                                    logger.info(f"   ğŸµ Ãudio: {'âœ…' if response_audio else 'âŒ'}")

                                except Exception as e:
                                    logger.error(f"âŒ Erro ao processar text_message: {e}")
                                    await ws.send_str(json.dumps({
                                        'type': 'error',
                                        'message': f'Erro ao processar mensagem: {str(e)}'
                                    }))
                            else:
                                logger.warning(f"âš ï¸ Texto vazio em text_message")
                                await ws.send_str(json.dumps({
                                    'type': 'error',
                                    'message': 'Mensagem de texto vazia'
                                }))

                        elif data.get('type') == 'text_to_speech':
                            # Mensagem de texto para TTS
                            text = data.get('text', '')
                            voice_id = data.get('voice_id', 'af_bella')
                            speed = data.get('speed', 1.0)
                            volume = data.get('volume', 1.0)

                            logger.info(f"ğŸ’¬ TTS solicitado: '{text[:50]}...' ({len(text)} chars)")
                            logger.info(f"ğŸµ Voice: {voice_id}, Speed: {speed}, Volume: {volume}")

                            if text.strip() and self.tts_module:
                                try:
                                    # Enviar status inicial
                                    await ws.send_str(json.dumps({
                                        'type': 'processing',
                                        'message': f'ğŸµ Processando com TTS ({voice_id})...',
                                        'text_length': len(text),
                                        'voice_id': voice_id
                                    }))

                                    # Set the voice before synthesis
                                    if hasattr(self.tts_module, 'set_voice'):
                                        self.tts_module.set_voice(voice_id)
                                        logger.info(f"ğŸ¯ Voice set to: {voice_id} before TTS synthesis")

                                    # Sintetizar com TTS
                                    tts_start = asyncio.get_event_loop().time()
                                    response_audio = await self.tts_module.synthesize(text, voice_id=voice_id, speed=speed)
                                    tts_time = (asyncio.get_event_loop().time() - tts_start) * 1000

                                    if response_audio is not None:
                                        logger.info(f"âœ… TTS gerado: {len(response_audio)} bytes em {tts_time:.0f}ms")

                                        # Converter PCM para WAV com headers apropriados
                                        logger.info(f"ğŸ”Š Convertendo PCM para WAV...")
                                        wav_audio = pcm_to_wav(response_audio, sample_rate=24000, channels=1, bits_per_sample=16)
                                        logger.info(f"âœ… WAV criado: {len(wav_audio)} bytes (de {len(response_audio)} PCM)")

                                        # Converter Ã¡udio WAV para base64
                                        audio_b64 = base64.b64encode(wav_audio).decode('utf-8')

                                        # Enviar resposta TTS
                                        await ws.send_str(json.dumps({
                                            'type': 'tts_response',
                                            'text': text,
                                            'audio_data': audio_b64,
                                            'audio_format': 'wav',
                                            'sample_rate': 24000,  # TTS default
                                            'is_final': True,
                                            'voice_id': voice_id,
                                            'processing_info': {
                                                'tts_latency_ms': round(tts_time),
                                                'audio_bytes': len(wav_audio),
                                                'pcm_bytes': len(response_audio),
                                                'voice': voice_id,
                                                'speed': speed,
                                                'volume': volume
                                            }
                                        }))
                                    else:
                                        logger.error(f"âŒ Falha na geraÃ§Ã£o TTS")
                                        await ws.send_str(json.dumps({
                                            'type': 'error',
                                            'message': 'Erro na sÃ­ntese de voz'
                                        }))

                                except Exception as e:
                                    logger.error(f"âŒ Erro no TTS: {e}")
                                    await ws.send_str(json.dumps({
                                        'type': 'error',
                                        'message': f'Erro na sÃ­ntese de voz: {str(e)}'
                                    }))
                            else:
                                if not text.strip():
                                    logger.warning(f"âš ï¸ Texto vazio para TTS")
                                    await ws.send_str(json.dumps({
                                        'type': 'error',
                                        'message': 'Texto vazio - digite algo para sintetizar'
                                    }))
                                elif not self.tts_module:
                                    logger.warning(f"âš ï¸ TTS nÃ£o disponÃ­vel")
                                    await ws.send_str(json.dumps({
                                        'type': 'error',
                                        'message': 'MÃ³dulo TTS nÃ£o configurado'
                                    }))
                        else:
                            logger.warning(f"âš ï¸  Tipo de mensagem nÃ£o suportado: {data.get('type')}")
                            
                    except json.JSONDecodeError as e:
                        logger.error(f"âŒ Erro ao decodificar JSON: {e}")
                    except Exception as e:
                        logger.error(f"âŒ Erro ao processar mensagem TEXT: {e}")
                
                elif msg.type == WSMsgType.ERROR:
                    logger.error(f'âŒ Erro WebSocket: {ws.exception()}')
                    
        except Exception as e:
            logger.error(f"âŒ Erro no WebSocket: {e}")
            
        finally:
            self.stats["active_connections"] -= 1
            # Remover conexÃ£o do set de mÃ©tricas
            self.websocket_connections.discard(ws)
            logger.info(f"ğŸ§¹ WebSocket desconectado: {session_id}")
            
        return ws
        
    async def serve_frontend(self, request) -> FileResponse:
        """Servir frontend React (index.html para qualquer rota)"""
        try:
            index_path = self.frontend_build_path / "index.html"
            if index_path.exists():
                return FileResponse(str(index_path))
            else:
                return web.Response(
                    text="Frontend nÃ£o encontrado. Execute: cd frontend && npm run build",
                    status=404
                )
        except Exception as e:
            logger.error(f"Erro servindo frontend: {e}")
            return web.Response(text="Erro interno", status=500)
        
    async def start(self) -> Any:
        """Iniciar servidor WebRTC"""
        logger.info(f"ğŸš€ Iniciando servidor WebRTC Python...")
        logger.info(f"ğŸ“¡ Escutando em {self.host}:{self.port}")
        logger.info(f"âš¡ Modo: Ultra-baixa latÃªncia com DataChannel")
        logger.info(f"ğŸ¯ LatÃªncia esperada: 25-40ms")
        
        runner = web.AppRunner(self.app)
        await runner.setup()
        site = web.TCPSite(runner, self.host, self.port)
        await site.start()
        
        # Monitor de stats
        asyncio.create_task(self.stats_monitor())
        
    async def stats_monitor(self) -> Any:
        """Monitor periÃ³dico de estatÃ­sticas"""
        while True:
            await asyncio.sleep(30)
            if self.stats["total_messages"] > 0:
                logger.info("ğŸ“Š === ESTATÃSTICAS ===")
                logger.info(f"ConexÃµes ativas: {self.stats['active_connections']}")
                logger.info(f"Total mensagens: {self.stats['total_messages']}")
                logger.info(f"LatÃªncia mÃ©dia: {self.stats['avg_latency']:.1f}ms")
                logger.info(f"LatÃªncia mÃ­n: {self.stats['min_latency']:.1f}ms")
                logger.info(f"LatÃªncia mÃ¡x: {self.stats['max_latency']:.1f}ms")


class WebRTCModule:
    """
    MÃ³dulo WebRTC para integraÃ§Ã£o com arquitetura Python
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        # âœ… Phase 3c: Dynamic port support via environment variable
        import os
        from src.config.service_config import ServiceType, get_service_port
        try:
            dynamic_port = int(os.getenv("WEBRTC_PORT") or get_service_port(ServiceType.WEBRTC_GATEWAY))
        except (OSError, ConnectionError, RuntimeError):
            dynamic_port = config.get("port", 8020)  # Fallback to config or PORT_MATRIX default

        self.server = WebRTCServer(
            host=config.get("host", "0.0.0.0"),
            port=dynamic_port,
            ice_servers=config.get("ice_servers", None)
        )
        
    async def initialize(self) -> Any:
        """Inicializar mÃ³dulo"""
        await self.server.start()
        
    def set_audio_processor(self, processor) -> Any:
        """Definir processador de Ã¡udio"""
        self.server.set_audio_processor(processor)
        
    def set_tts_module(self, tts) -> Any:
        """Definir mÃ³dulo TTS"""
        self.server.set_tts_module(tts)
        
    async def cleanup(self) -> Any:
        """Limpar recursos"""
        for session_id in list(self.server.peers.keys()):
            await self.server.cleanup_peer(session_id)


# Exemplo de uso standalone
async def main() -> Any:
    logging.basicConfig(level=logging.INFO)
    
    server = WebRTCServer()
    
    # Configurar mÃ³dulos (exemplo)
    # server.set_audio_processor(ultravox_module)
    # server.set_tts_module(tts_module)
    
    await server.start()
    
    # Manter rodando
    try:
        await asyncio.Event().wait()
    except KeyboardInterrupt:
        logger.info("ğŸ›‘ Encerrando servidor...")


if __name__ == "__main__":
    asyncio.run(main())